### Crawling

```
# Crawling, often called spidering, is the automated process of systematically browsing the World Wide Web.
# Similar to how a spider navigates its web, a web crawler follows links from one page to another, collecting information. 
# These crawlers are essentially bots that use pre-defined algorithms to discover and index web pages, 
# making them accessible through search engines or for other purposes like data analysis and web reconnaissance.

# It starts with a seed URL, which is the initial web page to crawl. 
# The crawler fetches this page, parses its content, and extracts all its links. 
# It then adds these links to a queue and crawls them, repeating the process iteratively.
# Depending on its scope and configuration, the crawler can explore an entire website or even a vast portion of the web.
```

```
# There are two primary types of crawling strategies:

- Breadth-First Crawling

- Depth-First Crawling
```

### Extracting Valuable Information

```
# Crawlers can extract a diverse array of data, each serving a specific purpose in the reconnaissance process:
- Links (Internal and External) 
- Comments
- Metadata
- Sensitive Files: backup files, configuration files, log files, API keys etc.
``` 

### robots.txt

```
# Technically, robots.txt is a simple text file placed in the root directory of a website (e.g., www.example.com/robots.txt). 
# It adheres to the Robots Exclusion Standard, guidelines for how web crawlers should behave when visiting a website. 
# This file contains instructions in the form of "directives" that tell bots which parts of the website they can and cannot crawl.
```

```
# How robots.txt Works
# The directives in robots.txt typically target specific user-agents, 
# which are identifiers for different types of bots. For example, a directive might look like this:

Code: txt
User-agent: *
Disallow: /private/
```

```
# Understanding robots.txt Structure

- User-agent: This line specifies which crawler or bot the following rules apply to. 
A wildcard (*) indicates that the rules apply to all bots.

- Directives: These lines provide specific instructions to the identified user-agent.

# Common directives include:

- Disallow        Disallow: /admin/ (disallow access to the admin directory)
- Allow           Allow: /public/ (allow access to the public directory)
- Crawl-delay     Crawl-delay: 10 (10-second delay between requests)
- Sitemap         Sitemap: https://www.example.com/sitemap.xml  (sitemap path for more efficient crawl)
```

```
# Why Respect robots.txt?
# While robots.txt is not strictly enforceable (a rogue bot could still ignore it), 
# most legitimate web crawlers and search engine bots will respect its directives. 

# This is important for several reasons:
- Avoiding Overburdening Servers
- Protecting Sensitive Information
- Legal and Ethical Compliance
```

### robots.txt in Web Reconnaissance

```
# For web reconnaissance, robots.txt serves as a valuable source of intelligence. 
# While respecting the directives outlined in this file, 
# security professionals can glean crucial insights into the structure and potential vulnerabilities of a target website:

- Uncovering Hidden Directories
Disallowed paths in robots.txt often point to directories or files the website owner intentionally wants to keep out of reach from search engine crawlers.

- Mapping Website Structure
By analyzing the allowed and disallowed paths, security professionals can create a rudimentary map of the website's structure.

- Detecting Crawler Traps
Some websites intentionally include "honeypot" directories in robots.txt to lure malicious bots.
```

```
# Analyzing robots.txt

Code: txt
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /public/

User-agent: Googlebot
Crawl-delay: 10

Sitemap: https://www.example.com/sitemap.xml


# All user agents are disallowed from accessing the /admin/ and /private/ directories.
# All user agents are allowed to access the /public/ directory.
# The Googlebot (Google's web crawler) is specifically instructed to wait 10 seconds between requests.
# The sitemap, located at https://www.example.com/sitemap.xml, is provided for easier crawling and indexing.
```

### Well-Known URIs

```
# The .well-known standard, defined in RFC 8615, serves as a standardized directory within a website's root domain. 
# This designated location, typically accessible via the /.well-known/ path on a web server, 
# centralizes a website's critical metadata, including configuration files and information related to its services, protocols, and security mechanisms.
```

```
# The Internet Assigned Numbers Authority (IANA) maintains a registry of .well-known URIs, 
# each serving a specific purpose defined by various specifications and standards. 

# Below is a table highlighting a few notable examples:

- security.txt                     Contains contact information for security researchers to report vulnerabilities.
- /.well-known/change-password     Provides a standard URL for directing users to a password change page.
- openid-configuration             Defines configuration details for OpenID Connect, an identity layer on top of the OAuth 2.0 protocol.
- assetlinks.json                  Used for verifying ownership of digital assets (e.g., apps) associated with a domain.
-  mta-sts.txt                     Specifies the policy for SMTP MTA Strict Transport Security (MTA-STS) to enhance email security.
``` 

### Web Recon and .well-known

```
# In web recon, the .well-known URIs can be invaluable for discovering endpoints and configuration details 
# that can be further tested during a penetration test. One particularly useful URI is openid-configuration.

# The openid-configuration URI is part of the OpenID Connect Discovery protocol, 
# an identity layer built on top of the OAuth 2.0 protocol.

# it can retrieve the OpenID Connect Provider's configuration by accessing the
https://example.com/.well-known/openid-configuration endpoint:

output:
Code: json
{
  "issuer": "https://example.com",
  "authorization_endpoint": "https://example.com/oauth2/authorize",
  "token_endpoint": "https://example.com/oauth2/token",
  "userinfo_endpoint": "https://example.com/oauth2/userinfo",
  "jwks_uri": "https://example.com/oauth2/jwks",
  "response_types_supported": ["code", "token", "id_token"],
  "subject_types_supported": ["public"],
  "id_token_signing_alg_values_supported": ["RS256"],
  "scopes_supported": ["openid", "profile", "email"]
}

[IANA](https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml) well-known URL's registry
```


### Creepy Crawlies

```
# Web crawling is vast and intricate, but you don't have to embark on this journey alone. 
# A plethora of web crawling tools are available to assist you, each with its own strengths and specialties.  

# Popular Web Crawlers
- Burp Suite Spider: Burp Suite, a widely used web application testing platform, includes a powerful active crawler called Spider.
- OWASP ZAP (Zed Attack Proxy): ZAP is a free, open-source web application security scanner.
- Scrapy (Python Framework): Scrapy is a versatile and scalable Python framework for building custom web crawlers.
- Apache Nutch (Scalable Crawler): Nutch is a highly extensible and scalable open-source web crawler written in Java.

# Always obtain permission before crawling a website, especially if you plan to perform extensive or intrusive scans. 
# Be mindful of the website's server resources and avoid overloading them with excessive requests.
```

### Scrapy

```
pip3 install scrapy     # Install tools

wget -O ReconSpider.zip https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip   # dl ReconSpider
unzip ReconSpider.zip   # unzip ReconSpider

python3 ReconSpider.py http://inlanefreight.com   # Start reconSpider (based in scrapy) against URL

# tool will output JSON result file with all crawled info.
e.g.:
Code: json
{
    "emails": [
        "lily.floid@inlanefreight.com",
        "cvs@inlanefreight.com",
        ...
    ],
    "links": [
        "https://www.themeansar.com",
        "https://www.inlanefreight.com/index.php/offices/",
        ...
    ],
    "external_files": [
        "https://www.inlanefreight.com/wp-content/uploads/2020/09/goals.pdf",
        ...
    ],
    "js_files": [
        "https://www.inlanefreight.com/wp-includes/js/jquery/jquery-migrate.min.js?ver=3.3.2",
        ...
    ],
    "form_fields": [],
    "images": [
        "https://www.inlanefreight.com/wp-content/uploads/2021/03/AboutUs_01-1024x810.png",
        ...
    ],
    "videos": [],
    "audio": [],
    "comments": [
        "<!-- #masthead -->",
        ...
    ]
}

# output json file keys:
emails	        Lists email addresses found on the domain.
links	        Lists URLs of links found within the domain.
external_files	Lists URLs of external files such as PDFs.
js_files	    Lists URLs of JavaScript files used by the website.
form_fields	    Lists form fields found on the domain (empty in this example).
images	        Lists URLs of images found on the domain.
videos	        Lists URLs of videos found on the domain (empty in this example).
audio	        Lists URLs of audio files found on the domain (empty in this example).
comments	    Lists HTML comments found in the source code.
```


